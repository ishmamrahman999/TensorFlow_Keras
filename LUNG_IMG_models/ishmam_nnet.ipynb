{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8f8a84",
   "metadata": {},
   "source": [
    "<h2> Define functions <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    # ReLU just outputs x if itâ€™s positive, otherwise 0\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax turns scores into probabilities\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2576cd",
   "metadata": {},
   "source": [
    "<h2>Load Test Data and Labels <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d1dd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data and class labels loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load test features (comma-separated values)\n",
    "    test_data = np.loadtxt('testdata.txt', delimiter=',')\n",
    "\n",
    "    # Load corresponding class labels (integers)\n",
    "    test_classes = np.loadtxt('test_class.txt', dtype=int)\n",
    "\n",
    "    print(\"Test data and class labels loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the required files. Please check your directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca259c6",
   "metadata": {},
   "source": [
    "<h2>Define weights<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f91bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are two sets of pre-trained weights for comparison\n",
    "weights_set1 = {\n",
    "    'w1': 1.3438, 'w2': -0.6225, 'w3': 0.3509, 'w4': -1.7072,\n",
    "    'w5': -1.1398, 'w6': 0.3944, 'w7': 1.3882, 'w8': -0.8676\n",
    "}\n",
    "\n",
    "weights_set2 = {\n",
    "    'w1': 0.8061, 'w2': 0.2354, 'w3': -0.4092, 'w4': -0.8999,\n",
    "    'w5': -0.5538, 'w6': -0.1916, 'w7': 0.0288, 'w8': 0.4918\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5caa2",
   "metadata": {},
   "source": [
    "<h2>Pediction and accuracy functions<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab54b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(data, true_labels, weights):\n",
    "    # Runs the model on each test input and checks how often it gets the right answer\n",
    "\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        x1, x2 = data[i]  # Grab the two input features\n",
    "\n",
    "        # Compute inputs to the hidden layer neurons (no bias used)\n",
    "        J1 = x1 * weights['w1'] + x2 * weights['w3']\n",
    "        J2 = x1 * weights['w2'] + x2 * weights['w4']\n",
    "\n",
    "        # Pass through ReLU\n",
    "        H1 = relu(J1)\n",
    "        H2 = relu(J2)\n",
    "\n",
    "        # Compute final layer inputs (again, no bias here)\n",
    "        I1 = H1 * weights['w5'] + H2 * weights['w7']\n",
    "        I2 = H1 * weights['w6'] + H2 * weights['w8']\n",
    "\n",
    "        # Softmax gives the class probabilities\n",
    "        output_probabilities = softmax(np.array([I1, I2]))\n",
    "        y1_prob, y2_prob = output_probabilities\n",
    "\n",
    "        # Pick whichever class has the higher probability\n",
    "        predicted_class = 0 if y1_prob >= y2_prob else 1\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "        # Track how many predictions were correct\n",
    "        if predicted_class == true_labels[i]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Accuracy is just correct guesses divided by total samples\n",
    "    accuracy = correct_predictions / data.shape[0]\n",
    "    return accuracy, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f291",
   "metadata": {},
   "source": [
    "<h2>Run prediction for both models<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7e2d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model with Set 1 Weights ---\n",
      "Prediction Accuracy for Set 1 weights: 0.9456\n",
      "\n",
      "--- Evaluating Model with Set 2 Weights ---\n",
      "Prediction Accuracy for Set 2 weights: 0.3233\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating Model with Set 1 Weights ---\")\n",
    "accuracy_set1, predictions_set1 = predict_accuracy(test_data, test_classes, weights_set1)\n",
    "print(f\"Prediction Accuracy for Set 1 weights: {accuracy_set1:.4f}\")\n",
    "\n",
    "print(\"\\n--- Evaluating Model with Set 2 Weights ---\")\n",
    "accuracy_set2, predictions_set2 = predict_accuracy(test_data, test_classes, weights_set2)\n",
    "print(f\"Prediction Accuracy for Set 2 weights: {accuracy_set2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3c64f",
   "metadata": {},
   "source": [
    "<h2>Result and conclusion<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127c4627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conclusion ---\n",
      "Set 1 performed better (0.9456 vs 0.3233)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Conclusion ---\")\n",
    "if accuracy_set1 > accuracy_set2:\n",
    "    print(f\"Set 1 performed better ({accuracy_set1:.4f} vs {accuracy_set2:.4f})\")\n",
    "elif accuracy_set2 > accuracy_set1:\n",
    "    print(f\"Set 2 performed better ({accuracy_set2:.4f} vs {accuracy_set1:.4f})\")\n",
    "else:\n",
    "    print(f\"Both sets had the same accuracy: {accuracy_set1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
