{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24dab6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading and Preprocessing ---\n",
      "Train shape: (28140, 1000, 4) Test shape: (2000, 1000, 4)\n",
      "\n",
      "--- Training Models ---\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, Input, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "print(\"--- Data Loading and Preprocessing ---\")\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_len):\n",
    "    mapping = {\n",
    "        'A': [1, 0, 0, 0], 'a': [1, 0, 0, 0],\n",
    "        'T': [0, 1, 0, 0], 't': [0, 1, 0, 0],\n",
    "        'G': [0, 0, 1, 0], 'g': [0, 0, 1, 0],\n",
    "        'C': [0, 0, 0, 1], 'c': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0], 'n': [0, 0, 0, 0]\n",
    "    }\n",
    "    encoded = [mapping.get(char, [0, 0, 0, 0]) for char in sequence]\n",
    "    encoded = np.array(encoded, dtype=np.float32)\n",
    "    if len(encoded) < max_len:\n",
    "        pad = np.zeros((max_len - len(encoded), 4), dtype=np.float32)\n",
    "        encoded = np.vstack((encoded, pad))\n",
    "    return encoded[:max_len]\n",
    "\n",
    "train_df = pd.read_csv('dm3.kc167.tads.train.csv', header=None)\n",
    "X_train_raw, y_train = train_df[0].values, train_df[1].values\n",
    "test_df = pd.read_csv('dm3.kc167.tads.test.csv', header=None)\n",
    "X_test_raw, y_test = test_df[0].values, test_df[1].values\n",
    "\n",
    "sequence_length = len(X_train_raw[0])\n",
    "X_train = np.array([one_hot_encode_sequence(seq, sequence_length) for seq in X_train_raw])\n",
    "X_test = np.array([one_hot_encode_sequence(seq, sequence_length) for seq in X_test_raw])\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "input_shape = (sequence_length, 4)\n",
    "\n",
    "# --- Callbacks (defined globally as they manage the training process, not model internal parameters) ---\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "print(\"\\n--- Training Models ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ef3005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model 1: Simple CNN ---\n",
      "Epoch 1/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.5514 - loss: 0.8122 - val_accuracy: 0.0027 - val_loss: 1.0319 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.6460 - loss: 0.7051 - val_accuracy: 0.3564 - val_loss: 0.9217 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7193 - loss: 0.6890 - val_accuracy: 0.5593 - val_loss: 0.8884 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.7693 - loss: 0.6695 - val_accuracy: 0.1501 - val_loss: 1.6729 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.8094 - loss: 0.6488 - val_accuracy: 0.2745 - val_loss: 1.5681 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.8387 - loss: 0.6381 - val_accuracy: 0.3826 - val_loss: 1.5536 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.8589 - loss: 0.6319 - val_accuracy: 0.6096 - val_loss: 1.1722 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8776 - loss: 0.6309\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8774 - loss: 0.6312 - val_accuracy: 0.2690 - val_loss: 1.7920 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9026 - loss: 0.6125 - val_accuracy: 0.4467 - val_loss: 1.4499 - learning_rate: 2.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9205 - loss: 0.5435 - val_accuracy: 0.4430 - val_loss: 1.5147 - learning_rate: 2.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9350 - loss: 0.4933 - val_accuracy: 0.5444 - val_loss: 1.2761 - learning_rate: 2.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9445 - loss: 0.4511 - val_accuracy: 0.5089 - val_loss: 1.4129 - learning_rate: 2.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9522 - loss: 0.4176\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9522 - loss: 0.4176 - val_accuracy: 0.5384 - val_loss: 1.4122 - learning_rate: 2.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9587 - loss: 0.3914 - val_accuracy: 0.4831 - val_loss: 1.5601 - learning_rate: 4.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9619 - loss: 0.3771 - val_accuracy: 0.4885 - val_loss: 1.5105 - learning_rate: 4.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9628 - loss: 0.3670 - val_accuracy: 0.5146 - val_loss: 1.4715 - learning_rate: 4.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9682 - loss: 0.3586 - val_accuracy: 0.4698 - val_loss: 1.6194 - learning_rate: 4.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9676 - loss: 0.3517\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9676 - loss: 0.3517 - val_accuracy: 0.4625 - val_loss: 1.5946 - learning_rate: 4.0000e-05\n",
      "Model 1 Test Accuracy: 0.7025\n"
     ]
    }
   ],
   "source": [
    "# Model 1 \tModel 1 – Simple CNN\n",
    "# Adjusted hyperparameters for Model 1 to improve test accuracy while managing overfitting\n",
    "print(\"\\n--- Model 1: Simple CNN ---\")\n",
    "# Model-specific hyperparameters\n",
    "model1_learning_rate = 0.001\n",
    "model1_epochs = 100\n",
    "model1_batch_size = 64\n",
    "model1_validation_split = 0.2\n",
    "model1_dropout_rate = 0.3 # Keeping dropout at 0.3\n",
    "model1_l2_strength = 0.0005 # Reverting L2 strength to 0.0005\n",
    "\n",
    "model1 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Conv1D(filters=64, kernel_size=9, activation='relu', kernel_regularizer=l2(model1_l2_strength)), # Increased filters from 32 to 64\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(model1_dropout_rate),\n",
    "    Flatten(),\n",
    "    Dense(units=64, activation='relu', kernel_regularizer=l2(model1_l2_strength)), # Increased Dense units from 32 to 64\n",
    "    Dropout(model1_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model1.compile(optimizer=Adam(model1_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(X_train, y_train, epochs=model1_epochs, batch_size=model1_batch_size, validation_split=model1_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 1 Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a524ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 Model 2 – Deeper CNN\n",
    "print(\"\\n--- Model 2: Deeper CNN ---\")\n",
    "# Model-specific hyperparameters\n",
    "model2_learning_rate = 0.001\n",
    "model2_epochs = 100\n",
    "model2_batch_size = 64\n",
    "model2_validation_split = 0.2\n",
    "model2_dropout_rate = 0.3\n",
    "model2_l2_strength = 0.0005\n",
    "\n",
    "model2 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Conv1D(filters=16, kernel_size=9, activation='relu', kernel_regularizer=l2(model2_l2_strength)),\n",
    "    MaxPooling1D(pool_size=2), Dropout(model2_dropout_rate),\n",
    "    Conv1D(filters=32, kernel_size=9, activation='relu', kernel_regularizer=l2(model2_l2_strength)),\n",
    "    MaxPooling1D(pool_size=2), Dropout(model2_dropout_rate),\n",
    "    Flatten(),\n",
    "    Dense(units=64, activation='relu', kernel_regularizer=l2(model2_l2_strength)),\n",
    "    Dropout(model2_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model2.compile(optimizer=Adam(model2_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs=model2_epochs, batch_size=model2_batch_size, validation_split=model2_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 2 Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41156cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 \tModel 3 – Simple LSTM\n",
    "print(\"\\n--- Model 3: Simple LSTM ---\")\n",
    "# Model-specific hyperparameters\n",
    "model3_learning_rate = 0.001\n",
    "model3_epochs = 100\n",
    "model3_batch_size = 64\n",
    "model3_validation_split = 0.2\n",
    "model3_dropout_rate = 0.3\n",
    "model3_l2_strength = 0.0005\n",
    "\n",
    "model3 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Bidirectional(LSTM(units=10)),\n",
    "    Dropout(model3_dropout_rate),\n",
    "    Dense(units=32, activation='relu', kernel_regularizer=l2(model3_l2_strength)),\n",
    "    Dropout(model3_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model3.compile(optimizer=Adam(model3_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs=model3_epochs, batch_size=model3_batch_size, validation_split=model3_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 3 Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 Model 4 – Deeper LSTM\n",
    "print(\"\\n--- Model 4: Deeper LSTM ---\")\n",
    "# Model-specific hyperparameters\n",
    "model4_learning_rate = 0.001\n",
    "model4_epochs = 100\n",
    "model4_batch_size = 64\n",
    "model4_validation_split = 0.2\n",
    "model4_dropout_rate = 0.3\n",
    "model4_l2_strength = 0.0005\n",
    "\n",
    "model4 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Bidirectional(LSTM(units=10, return_sequences=True)),\n",
    "    Dropout(model4_dropout_rate),\n",
    "    Bidirectional(LSTM(units=10)),\n",
    "    Dropout(model4_dropout_rate),\n",
    "    Dense(units=64, activation='relu', kernel_regularizer=l2(model4_l2_strength)),\n",
    "    Dropout(model4_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model4.compile(optimizer=Adam(model4_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model4.fit(X_train, y_train, epochs=model4_epochs, batch_size=model4_batch_size, validation_split=model4_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model4.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 4 Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b280ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 CNN-LSTM Hybrid\n",
    "print(\"\\n--- Model 5: CNN-LSTM Hybrid ---\")\n",
    "# Model-specific hyperparameters\n",
    "model5_learning_rate = 0.001\n",
    "model5_epochs = 100\n",
    "model5_batch_size = 64\n",
    "model5_validation_split = 0.2\n",
    "model5_dropout_rate = 0.3\n",
    "model5_l2_strength = 0.0005\n",
    "\n",
    "model5 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Conv1D(filters=32, kernel_size=9, activation='relu', kernel_regularizer=l2(model5_l2_strength)),\n",
    "    MaxPooling1D(pool_size=2), Dropout(model5_dropout_rate),\n",
    "    Bidirectional(LSTM(units=10)),\n",
    "    Dropout(model5_dropout_rate),\n",
    "    Dense(units=32, activation='relu', kernel_regularizer=l2(model5_l2_strength)),\n",
    "    Dropout(model5_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model5.compile(optimizer=Adam(model5_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model5.fit(X_train, y_train, epochs=model5_epochs, batch_size=model5_batch_size, validation_split=model5_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model5.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 5 Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11253675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6 CNN with More Dense Layers\n",
    "print(\"\\n--- Model 6: CNN with More Dense Layers ---\")\n",
    "# Model-specific hyperparameters\n",
    "model6_learning_rate = 0.001\n",
    "model6_epochs = 100\n",
    "model6_batch_size = 64\n",
    "model6_validation_split = 0.2\n",
    "model6_dropout_rate = 0.3\n",
    "model6_l2_strength = 0.0005\n",
    "\n",
    "model6 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Conv1D(filters=32, kernel_size=9, activation='relu', kernel_regularizer=l2(model6_l2_strength)),\n",
    "    MaxPooling1D(pool_size=2), Dropout(model6_dropout_rate), Flatten(),\n",
    "    Dense(units=64, activation='relu', kernel_regularizer=l2(model6_l2_strength)),\n",
    "    Dropout(model6_dropout_rate),\n",
    "    Dense(units=32, activation='relu', kernel_regularizer=l2(model6_l2_strength)),\n",
    "    Dropout(model6_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model6.compile(optimizer=Adam(model6_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model6.fit(X_train, y_train, epochs=model6_epochs, batch_size=model6_batch_size, validation_split=model6_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model6.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 6 Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7 LSTM with More Dense Layers\n",
    "print(\"\\n--- Model 7: LSTM with More Dense Layers ---\")\n",
    "# Model-specific hyperparameters\n",
    "model7_learning_rate = 0.001\n",
    "model7_epochs = 100\n",
    "model7_batch_size = 64\n",
    "model7_validation_split = 0.2\n",
    "model7_dropout_rate = 0.3\n",
    "model7_l2_strength = 0.0005\n",
    "\n",
    "model7 = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Bidirectional(LSTM(units=10)),\n",
    "    Dropout(model7_dropout_rate),\n",
    "    Dense(units=64, activation='relu', kernel_regularizer=l2(model7_l2_strength)),\n",
    "    Dropout(model7_dropout_rate),\n",
    "    Dense(units=32, activation='relu', kernel_regularizer=l2(model7_l2_strength)),\n",
    "    Dropout(model7_dropout_rate),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model7.compile(optimizer=Adam(model7_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model7.fit(X_train, y_train, epochs=model7_epochs, batch_size=model7_batch_size, validation_split=model7_validation_split,\n",
    "           callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "loss, accuracy = model7.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Model 7 Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
